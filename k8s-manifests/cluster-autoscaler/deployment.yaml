apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cluster-autoscaler
      containers:
        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.31.0
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/concur-test-eks
            - --balance-similar-node-groups
            - --skip-nodes-with-system-pods=false
            # Scale-up configuration (default values specified explicitly)
            - --max-node-provision-time=15m  # Max time to wait for node provisioning (default: 15m)
            - --max-total-unready-percentage=45  # Max percentage of unready nodes in cluster (default: 45)
            - --ok-total-unready-count=3  # Number of allowed unready nodes (default: 3)
            - --new-pod-scale-up-delay=0s  # Delay before scaling up for new pods (default: 0s)
            - --max-empty-bulk-delete=10  # Max empty nodes deleted at once (default: 10)
            # Scale-down configuration (default values specified explicitly)
            - --scale-down-enabled=true  # Enable scale-down (default: true)
            - --scale-down-utilization-threshold=0.5  # Scale down if node utilization < 50% (default: 0.5)
            - --scale-down-delay-after-add=10m  # Wait 10min after scale-up before scale-down (default: 10m)
            - --scale-down-delay-after-delete=10m  # Wait 10min after node deletion before next scale-down (default: 0s, recommended: 10m to prevent thrashing)
            - --scale-down-delay-after-failure=3m  # Wait 3min after scale-down failure (default: 3m)
            - --scale-down-unneeded-time=10m  # Node must be unneeded for 10min before scale-down (default: 10m)
            - --scale-down-unready-time=20m  # Unready node will be removed after 20min (default: 20m)
            - --max-graceful-termination-sec=600  # Max time to wait for pod termination (default: 600s)
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: "Always"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
